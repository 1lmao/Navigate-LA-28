services:
  client:
    container_name: navigate_la_frontend
    image: node:18
    working_dir: /app
    ports:
      - "3030:3030"
    volumes:
      - ./client:/app
    command: >
      bash -c "npm install &&
               npm install --save react-app-rewired crypto-browserify stream-browserify os-browserify path-browserify &&
               npm run start"
    depends_on:
      - server
    networks:
      - navigate_la_28
    environment:
      - WATCHPACK_POLLING=true
      - WDS_SOCKET_PORT=3030

  server:
    container_name: navigate_la_backend
    build: ./server
    ports:
      - "8000:8000"
    volumes:
      - ./server:/app
      - ./server/datasets:/app/datasets
    environment:
      - PYTHONPATH=/app
    networks:
      - navigate_la_28
    depends_on:
      - postgres
      - postgres_test
    command: >
      bash -c "
        python models/init_db.py &&
        uvicorn main:app --host 0.0.0.0 --port 8000 --reload
      "

  hadoop:
    container_name: hadoop
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    environment:
      - CLUSTER_NAME=test
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./hadoop/conf:/opt/hadoop-3.2.1/etc/hadoop
    networks:
      - navigate_la_28

  spark:
    container_name: spark_master
    build:
      context: ./spark
      dockerfile: Dockerfile # Custom Dockerfile to add Hadoop
    environment:
      - SPARK_MODE=standalone
      - DATASET_DIR=/datasets # Path to datasets in the container
    ports:
      - "7077:7077" # Spark Master Port
      - "8080:8080" # Spark Master Web UI
    volumes:
      - ./spark/conf:/opt/bitnami/spark/conf
    networks:
      - navigate_la_28
    command: >
      bash -c "
        /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.master.Master --host 0.0.0.0 --port 7077 &&
        bash /opt/move_to_hdfs.sh &&
        /opt/bitnami/spark/bin/spark-sql
      "

  spark-worker:
    container_name: spark_worker
    image: bitnami/spark:3.3.0
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
    depends_on:
      - spark
    networks:
      - navigate_la_28
    ports:
      - "8081:8081" # Spark Worker Web UI
    command: >
      bash -c "
        /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark:7077
      "

  postgres:
    container_name: navigate_la_postgres
    image: postgres:13
    environment:
      POSTGRES_USER: la28_user
      POSTGRES_PASSWORD: bigdata_la28
      POSTGRES_DB: navigate_la28_db
    ports:
      - "5433:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - navigate_la_28

  postgres_test:
    container_name: navigate_la_postgres_test
    image: postgres:13
    environment:
      POSTGRES_USER: la28_user
      POSTGRES_PASSWORD: bigdata_la28
      POSTGRES_DB: navigate_la28_test_db
    ports:
      - "5434:5432" # Use a different port to avoid conflicts with the production database
    volumes:
      - postgres_test_data:/var/lib/postgresql/data
    networks:
      - navigate_la_28

volumes:
  hadoop_namenode:
  postgres_data:
  postgres_test_data:

networks:
  navigate_la_28:
